# Lecture 9 — Linear Regression



本节课介绍了线性回归的一些基本性质，任然引入信用卡的例子，来解决给用户发放信用卡的额度问题，这就是一个线性回归问题。



首先令用户的特征集合为$d$ 维的$X$ ，再加上常数项， 总的维度为$d + 1$ ， 与权重$w$ 的线性组合就是我们的假设（Hypothesis），记为$h(x)$ 。线性回归的预测值是整个实数空间，这与线性分类不同。
$$
h(x) = w^TX
$$


![linearRegression-2](D:\机器学习基石笔记\linearRegression-2.png)

**residuals** : **残差**

上面两张图，第一张是一个二维空间中的找到一条直线，再三维空间中找到一个平面或者更高维度中的超平面，目的是让样本中的点更接近它，也就是这条直线或这个平面，从计算上来说也就是残差Residuals最小化。



一般常用的错误测量方法是基于***最小二乘法*** ，即通过最小化误差的平方和寻找数据的最佳函数匹配，目标是计算最小平方和对应的权重w。

为了减少由于极端数据的影响而造成的巨大波动，通常采用类似方差来减少个别数据影响，至于选择平方和作为估计函数，则需从概率分布角度了解其公式来源（统计学中，**残差平方和函数**可以看成n倍的均方误差），除以N则计算平均值，由此导出以下代价函数（cost Function）：
$$
E_{in}(w)=\frac{1}{N}\sum_{n=1}^N(w^Tx_n-y_n)^2
$$
我解释一下这其中的各个参数的含义：$E_{in}$代表训练集的表现，w是要求的权重，y是我们已知的，类比信用卡例子，x就是代表所有用户的信息，y代表银行对于特定用户的信用卡额度，那么w是我们为了建立预测模型所需求的一个权重，这样对于一个全新的用户，我们就可以给出一个相对正确的y（信用额度）。因此这个我们的目的就是求出一组w使得上式的值最小，也就是更接近于实际情况。

接下来为了求得最小值，我们做一些计算上的预处理，简化一下这个式子，首先我们看到$w^Tx_n$ 其中w和x都是向量他们地积是一个常数，那么由于向量内积的数学原理，所以$w^Tx_n=x_n^Tw$ 。

又由于平方的累加等于向量求L2范数后再平方因此：
$$
\begin{equation}
\begin{aligned}
E_{in}(w)&=\frac{1}{N}
\left\|
	\begin{matrix}
		\vec{x}_1^Tw-y_1\\ 
		\vec{x}_2^Tw-y_2\\ 
		\cdots\\ 
		\vec{x}_n^Tw-y_n
    \end{matrix}
\right\|^2\\
&=\frac{1}{N}
\left\|
	\begin{bmatrix}
		--x^T_1--\\
		--x^T_2--\\
		\cdots\\
		--x^T_N--
	\end{bmatrix}
	w-
	\begin{bmatrix}
		y_1\\
		y_2\\
		\cdots\\
		y_N
	\end{bmatrix}
\right\|^2\\

&=\frac{1}{N}
	\left\|
		\underbrace{X}_{N\times (d+1)}\underbrace{w}_{(d+1)\times 1}-
		\underbrace{y}_{N\times 1}
	\right\|^2
\end{aligned}
\end{equation}
$$



最终转化为：
$$
\min \limits_w E_{in}(w)=\frac{1}{N}\left\|Xw-y\right\|^2
$$
那么对于此类线性回归问题，$E_{in}(w)$一般是一个凸函数。对于凸函数，我们找到一阶导数为零的位置，就找到了最优解。那么我们分别将$E_w 对每个w_i, i = 0,1,\cdots,d$ 求偏导，偏导为0的$w_i$，即为最优化的权重值方案。

那么此处的$\left\|Xw-y\right\|^2$这是$xw-y$的L2范数的平方

也就是向量中所有元素的平方和也可以理解为向量的 **内积** 即：$A^TA$

因此:
$$
E_{in}(w)=\frac{1}{N}\left\|Xw-y\right\|^2=\frac{1}{N}\Big(\mathop{w^TX^TXw}\limits_{A} -\mathop{2w^TX^Ty}\limits_b+\mathop{y^Ty}\limits_c \Big) \tag{1}
$$
其中$\left\|Xw-y\right\|^2$ 就是$(Xw-y)^T(Xw-y)$ 

根据转置矩阵公式$(AB)^T=B^TA^T;(A+B)^T=A^T+B^T$得：
$$
\begin{aligned}
(w^TX^T-y^T)(Xw-y)&=(w^TX^TXw-w^TX^Ty-y^TXw-y^Ty)\\
&=(w^TX^TXw-w^TX^Ty-w^TX^Ty-y^Ty)\\
&=(w^TX^TXw-2w^TX^Ty-y^Ty)=(1)
\end{aligned}
$$
然后对(1)式进行求偏导：
$$
\begin{aligned}
\nabla E_{in}(w)=\frac{\partial E(w)}{\partial w}&=
 \frac{\partial (w^TX^TXw-2w^TX^Ty-y^Ty)}
 	{\partial w}\\
 &=\frac{\partial(w^TX^TXw)}{\partial w}-\frac{\partial(2w^TX^Ty)}{\partial w}-
 \frac{\partial(y^Ty)}{\partial w}\\
 &=
\end{aligned}
$$
